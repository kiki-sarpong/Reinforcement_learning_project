{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym  # open ai gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout,Activation\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_QN:\n",
    "    def __init__(self,env,model_available,file_name):\n",
    "        self.model_env = env                     \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.min_epsilon = 0.01\n",
    "        self.dec_epsilon = 0.996\n",
    "        self.learning_rate = 0.0005\n",
    "        self.model_available = model_available\n",
    "        self.weights_file = file_name\n",
    "        self.batch_size = 32\n",
    "        self.max_memory = 50_000\n",
    "        self.memory = deque(maxlen=self.max_memory)\n",
    "        if model_available:\n",
    "            self.model = load_model(self.weights_file)\n",
    "            self.model_target = load_model(self.weights_file)\n",
    "        else:\n",
    "            self.model = self.BuildModel()\n",
    "            self.model_target = self.BuildModel()\n",
    "\n",
    "        \n",
    "    def BuildModel(self):\n",
    "        model_input_shape = self.model_env.observation_space.shape       #model input shape for  input layer\n",
    "        model_output_shape = self.model_env.action_space.n              #model output shape for output layer\n",
    "        model = Sequential()\n",
    "        model.add(Dense(512, input_dim=model_input_shape[0],activation=\"relu\"))\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.model_env.action_space.n,activation=\"linear\"))                   #output layer with num of outputs equal to action space\n",
    "        model.compile(loss=\"mean_squared_error\",optimizer=Adam(lr=self.learning_rate))        #compile model and return\n",
    "        return model\n",
    "    \n",
    "    def Recall(self, pres_state, action, reward, future_state, done):     #Keep track of states,action....... in memory     \n",
    "        self.memory.append([pres_state,action,reward,future_state,done])\n",
    "\n",
    "    def ModelSave(self):      #save the model under specified name\n",
    "        self.model.save(self.weights_file)       ##save model in specified weights file\n",
    "    \n",
    "    def Replay(self):\n",
    "        if len(self.memory) < self.batch_size:           #if memory isn't up to batch size yet return.\n",
    "            return\n",
    "        sample_minibatch = random.sample(self.memory,self.batch_size)       #select random batch from memory\n",
    "\n",
    "        for sm in sample_minibatch:                      #iterate through the sampled memory\n",
    "            pres_state, action, reward, future_state, done = sm\n",
    "            pres_state = pres_state[np.newaxis,:]         #reshape states\n",
    "            future_state = future_state[np.newaxis,:]  \n",
    "            target_network = self.model_target.predict(pres_state)\n",
    "            tn = self.model_target.predict(future_state) \n",
    "            status = not done                                                      #invert done \n",
    "            target_network[0,action] = reward + self.gamma*np.max(tn[0])*status     #calculate reward for target network\n",
    "            self.model.fit(pres_state,target_network,epochs=1,verbose=0)           #train the model\n",
    "        self.epsilon = self.epsilon*self.dec_epsilon        #decay epsilon\n",
    "        if self.epsilon < self.min_epsilon:     #if the epsilon value is past the min threshold, use the min threshold\n",
    "            self.epsilon = self.min_epsilon\n",
    "            return self.epsilon\n",
    "        \n",
    "        \n",
    "    def ActionChoice(self,states):\n",
    "        states = states[np.newaxis,:]            #reshape states by using newaxis\n",
    "        rd = np.random.random()                 #select random value\n",
    "        if rd < self.epsilon:\n",
    "            action_select = self.model_env.action_space.sample()     #samples a random action\n",
    "        else:\n",
    "            new_action = self.model.predict(states)           #predict based on states\n",
    "            action_select = np.argmax(new_action)              #select maximum action\n",
    "        return action_select\n",
    "    \n",
    "            \n",
    "    def T_network(self):\n",
    "        model_theta = self.model.get_weights()                #get model weights\n",
    "        self.model_target.set_weights(model_theta)            #set model weights to target model\n",
    "        \n",
    "        \n",
    "    def plot(self,score_history,num_of_episodes):\n",
    "        history = np.array(score_history)              #convert list to numpy\n",
    "        avg = 20\n",
    "        c_sum = np.cumsum(score_history)               #calculates the cumulative sum of the score history array\n",
    "        plt.style.use(['dark_background'])\n",
    "        plt.rcParams[\"figure.figsize\"] = (12,5)        #change plot size\n",
    "        plt.plot(history)\n",
    "        plt.title('Plot of Rewards Distribution over %d Complete Episodes'% num_of_episodes)\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.xlabel('Episodes')\n",
    "        c_sum[avg:] = c_sum[avg:] - c_sum[:-avg]     \n",
    "        plt.plot(c_sum[avg - 1:] / avg)\n",
    "        plt.show()\n",
    "        mn = np.sum(history)/num_of_episodes\n",
    "        print(\"Mean value over complete episodes is {}.\".format(mn))\n",
    "        print(\"Highest reward is:{}.\".format(np.max(history)))\n",
    "        print(\"Lowest reward is:{}.\".format(np.min(history)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0519 14:59:39.022083 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0519 14:59:39.053997 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0519 14:59:39.200198 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0519 14:59:39.201228 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0519 14:59:39.204187 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0519 14:59:39.348798 29736 deprecation_wrapper.py:119] From C:\\Users\\kikiy\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = \"AcrobatModel_results3.h5\"      #filename for saving weights\n",
    "model_available = False                     #set to false\n",
    "if os.path.isfile(file_name):\n",
    "    model_available = True\n",
    "env_Acrobot = gym.make('Acrobot-v1')         #initialize environment and agent.\n",
    "DQN = Deep_QN(env_Acrobot,model_available,file_name)  #initiate Deep_QN class and pass parameters\n",
    "num_of_episodes = 5000                             #number of episodes\n",
    "score_history = []                                  #score history list \n",
    "update_c = 60                                    #number of iterations before the target network can be updated                                      \n",
    "                                         \n",
    "\n",
    "for run in range(num_of_episodes):                 #run episodes\n",
    "    score_num,ind = 0,0                         #initialize score and ind to zero\n",
    "    pres_state = env_Acrobot.reset()            \n",
    "    done = False\n",
    "    while not done:\n",
    "        env_Acrobot.render() # call this before env.reset, if you want a window showing the environment\n",
    "        action = DQN.ActionChoice(pres_state)\n",
    "        future_state,reward,done,info = env_Acrobot.step(action)\n",
    "        score_num += reward                                       #increment reward until done is true\n",
    "        DQN.Recall(pres_state,action,reward,future_state,done)    #save specified to memory \n",
    "        pres_state = future_state             #set state x to state x+1\n",
    "        ind = ind + 1                         #counter\n",
    "        if (ind%update_c) == 0:               #update target model after every specified update_c iterations\n",
    "            DQN.T_network()\n",
    "    eps = DQN.Replay()\n",
    "    if (run%15)== 0:                           #save the model after every 20 episode iterations\n",
    "        DQN.ModelSave()\n",
    "    score_history.append(score_num)             #store rewards\n",
    "    print(\"Episode number is:{} and Reward is:{}\".format(run,score_num))   #print episode number and corresponding reward\n",
    "DQN.plot(score_history,num_of_episodes)        #plot rewards received\n",
    "DQN.ModelSave()                                #save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
